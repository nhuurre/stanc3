  $ ../../../../../../install/default/bin/stanc --auto-format Nmix0.stan
// Open-population binomial-mixuture model
functions {
  /**
  * Returns log likelihood of N-mixture model
  * with 2 replicated observations using
  * bivariate Poisson distibution
  *
  * References
  * Dennis et al. (2015) Computational aspects of N-mixture models.
  *   Biometrics 71:237--246. DOI:10.1111/biom.12246
  * Stan users mailing list
  *   https://groups.google.com/forum/#!topic/stan-users/9mMsp1oB69g
  *
  * @param n          Number of observed individuals
  * @param log_lambda Log of Poisson mean of population size
  * @param p          Detection probability
  *
  * return Log probability
  */
  real bivariate_poisson_log_lpmf(int[] n, real log_lambda, real p) {
    real s[min(n) + 1];
    real log_theta_1 = log_lambda + log(p) + log1m(p);
    real log_theta_0 = log_lambda + log(p) * 2;
    if (size(n) != 2) 
      reject("Size of n must be 2.");
    if (p < 0 || p > 1) 
      reject("p must be in [0,1].");
    for (u in 0 : min(n)) 
      s[u + 1] = poisson_log_lpmf(n[1] - u| log_theta_1)
                 + poisson_log_lpmf(n[2] - u| log_theta_1)
                 + poisson_log_lpmf(u| log_theta_0);
    return log_sum_exp(s);
  }
}
data {
  int<lower=1> R;
  // Number of sites
  int<lower=1> T;
  // Number of replications; fixed as 2
  int<lower=-1> y[R, 2, 7];
  // Counts (-1:NA)
  int<lower=1, upper=7> first[R];
  // First occasion
  int<lower=1, upper=7> last[R];
  // Last occasion
  int<lower=1> K;
}
transformed data {
  int<lower=0> max_y[R, 7];
  for (i in 1 : R) {
    for (k in 1 : (first[i] - 1)) 
      max_y[i, k] = 0;
    for (k in (last[i] + 1) : 7) 
      max_y[i, k] = 0;
    for (k in first[i] : last[i]) 
      max_y[i, k] = max(y[i, 1 : T, k]);
  }
}
parameters {
  vector[7] alpha_lam;
  vector<lower=0, upper=1>[7] p;
  // Capture probability
}
model {
  // Priors
  alpha_lam ~ normal(0, 10);
  // A flat prior Uniform(0, 1) is implicitly used on p.
  // Likelihood
  for (i in 1 : R) 
    for (k in first[i] : last[i]) 
      y[i, 1 : T, k] ~ bivariate_poisson_log(alpha_lam[k], p[k]);
}
generated quantities {
  int totalN[7];
  real fit = 0;
  real fit_new = 0;
  vector[7] mean_abundance;
  {
    int N[R, 7];
    // Abundance
    real eval[R, 7];
    // Expected values
    int y_new[R, T, 7];
    matrix[T, 7] E[R];
    matrix[T, 7] E_new[R];
    // Initialize N, E and E_new
    N = rep_array(0, R, 7);
    E[1] = rep_matrix(0, T, 7);
    E_new[1] = rep_matrix(0, T, 7);
    for (i in 2 : R) {
      E[i] = E[i - 1];
      E_new[i] = E_new[i - 1];
    }
    for (i in 1 : R) {
      for (k in first[i] : last[i]) {
        vector[K + 1] lp;
        for (n in 0 : (max_y[i, k] - 1)) 
          lp[n + 1] = negative_infinity();
        for (n in max_y[i, k] : K) 
          lp[n + 1] = poisson_log_lpmf(n| alpha_lam[k])
                      + binomial_lpmf(y[i, 1 : T, k]| n, p[k]);
        N[i, k] = categorical_rng(softmax(lp)) - 1;
        eval[i, k] = p[k] * N[i, k];
        for (j in 1 : T) {
          // Assess model fit using Chi-squared discrepancy
          // Compute fit statistic E for observed data
          E[i, j, k] = square(y[i, j, k] - eval[i, k]) / (eval[i, k] + 0.5);
          // Generate replicate data and
          // Compute fit statistic E_new for replicate data
          y_new[i, j, k] = binomial_rng(N[i, k], p[k]);
          E_new[i, j, k] = square(y_new[i, j, k] - eval[i, k])
                           / (eval[i, k] + 0.5);
        }
      }
    }
    for (k in 1 : 7) 
      totalN[k] = sum(N[1 : R, k]);
    // Total pop. size across all sites
    for (i in 1 : R) {
      fit = fit + sum(E[i]);
      fit_new = fit_new + sum(E_new[i]);
    }
  }
  mean_abundance = exp(alpha_lam);
}

  $ ../../../../../../install/default/bin/stanc --auto-format Nmix1.stan
// Zero-inflated Poisson binomial-mixture model
functions {
  /**
  * Returns log likelihood of N-mixture model
  * with 2 replicated observations using
  * bivariate Poisson distibution
  *
  * References
  * Dennis et al. (2015) Computational aspects of N-mixture models.
  *   Biometrics 71:237--246. DOI:10.1111/biom.12246
  * Stan users mailing list
  *   https://groups.google.com/forum/#!topic/stan-users/9mMsp1oB69g
  *
  * @param n          Number of observed individuals
  * @param log_lambda Log of Poisson mean of population size
  * @param p          Detection probability
  *
  * return Log probability
  */
  real bivariate_poisson_log_lpmf(int[] n, real log_lambda, real p) {
    real s[min(n) + 1];
    real log_theta_1 = log_lambda + log(p) + log1m(p);
    real log_theta_0 = log_lambda + log(p) * 2;
    if (size(n) != 2) 
      reject("Size of n must be 2.");
    if (p < 0 || p > 1) 
      reject("p must be in [0,1].");
    for (u in 0 : min(n)) 
      s[u + 1] = poisson_log_lpmf(n[1] - u| log_theta_1)
                 + poisson_log_lpmf(n[2] - u| log_theta_1)
                 + poisson_log_lpmf(u| log_theta_0);
    return log_sum_exp(s);
  }
  /**
  * Return log probability of Poisson Binomial Mixture
  *
  * @param y          Count
  * @param n          Population size
  * @param log_lambda Log of Poisson mean
  * @param p          Detection probability
  *
  * @return Log probability
  */
  real poisbin_lpmf(int[] y, int n, real log_lambda, real p) {
    if (max(y) > n) 
      return negative_infinity();
    return poisson_log_lpmf(n| log_lambda) + binomial_lpmf(y| n, p);
  }
}
data {
  int<lower=1> R;
  // Number of sites
  int<lower=1> T;
  // Number of replications; fixed as 2
  int<lower=-1> y[R, 2, 7];
  // Counts (-1:NA)
  int<lower=1, upper=7> first[R];
  // First occasion
  int<lower=1, upper=7> last[R];
  // Last occasion
  int<lower=0> K;
  // Upper bounds of population size
}
transformed data {
  int<lower=0> max_y[R, 7];
  int<lower=0> max_y_site[R];
  for (i in 1 : R) {
    for (k in 1 : (first[i] - 1)) 
      max_y[i, k] = 0;
    for (k in (last[i] + 1) : 7) 
      max_y[i, k] = 0;
    for (k in first[i] : last[i]) 
      max_y[i, k] = max(y[i, 1 : T, k]);
    max_y_site[i] = max(max_y[i]);
  }
}
parameters {
  real<lower=0, upper=1> omega;
  // Suitability
  vector[7] alpha_lam;
  // Log abundance
  vector<lower=0, upper=1>[7] p;
  // Captue probability
}
model {
  // Priors
  // Implicit flat priors [0, 1] are used on omega and p.
  alpha_lam ~ normal(0, 10);
  // Likelihood
  for (i in 1 : R) {
    if (max_y_site[i]) {
      real lp = bernoulli_lpmf(1| omega);
      for (k in first[i] : last[i]) 
        lp = lp
             + bivariate_poisson_log_lpmf(y[i, 1 : T, k]| alpha_lam[k], p[k]);
      target += lp;
    }
    else {
      real lp[2];
      lp[1] = bernoulli_lpmf(0| omega);
      lp[2] = bernoulli_lpmf(1| omega);
      for (k in first[i] : last[i]) 
        lp[2] = lp[2]
                + bivariate_poisson_log_lpmf(y[i, 1 : T, k]| alpha_lam[k], p[k]);
      target += log_sum_exp(lp);
    }
  }
}
generated quantities {
  int totalN[7];
  // Total pop. size across all sites
  real fit = 0;
  real fit_new = 0;
  vector[7] mean_abundance;
  {
    int N[R, 7];
    // Latent abundance state
    real eval[R, 7];
    // Expected values
    int y_new[R, T, 7];
    matrix[T, 7] E[R];
    matrix[T, 7] E_new[R];
    // Initialize N, E and E_new
    N = rep_array(0, R, 7);
    E[1] = rep_matrix(0, T, 7);
    E_new[1] = rep_matrix(0, T, 7);
    for (i in 2 : R) {
      E[i] = E[i - 1];
      E_new[i] = E_new[i - 1];
    }
    for (i in 1 : R) {
      real log_p_unobs;
      // Log of prob. site is suitable
      // but no indiv. observed.
      for (k in first[i] : last[i]) {
        vector[K + 1] lp;
        for (n in 0 : K) 
          lp[n + 1] = poisbin_lpmf(y[i, 1 : T, k]| n, alpha_lam[k], p[k]);
        N[i, k] = categorical_rng(softmax(lp)) - 1;
      }
      if (max_y_site[i] == 0) {
        // Unobserved
        log_p_unobs = log(omega) + binomial_lpmf(0| N[i], p) * T;
        if (bernoulli_rng(exp(log_p_unobs)) == 0) {
          // Site is not suitable
          for (k in first[i] : last[i]) 
            N[i, k] = 0;
        }
      }
      for (k in first[i] : last[i]) {
        eval[i, k] = p[k] * N[i, k];
        for (j in 1 : T) {
          // Assess model fit using Chi-squared discrepancy
          // Compute fit statistic E for observed data
          E[i, j, k] = square(y[i, j, k] - eval[i, k]) / (eval[i, k] + 0.5);
          // Generate replicate data and compute fit stats for them
          y_new[i, j, k] = binomial_rng(N[i, k], p[k]);
          E_new[i, j, k] = square(y_new[i, j, k] - eval[i, k])
                           / (eval[i, k] + 0.5);
        }
      }
    }
    for (k in 1 : 7) 
      totalN[k] = sum(N[1 : R, k]);
    for (i in 1 : R) {
      fit = fit + sum(E[i]);
      fit_new = fit_new + sum(E_new[i]);
    }
  }
  mean_abundance = exp(alpha_lam);
}

  $ ../../../../../../install/default/bin/stanc --auto-format Nmix2.stan
// Binomial-mixture model with overdispersion in both abundance and detection
functions {
  /**
  * Returns log likelihood of N-mixture model
  * with 2 replicated observations using
  * bivariate Poisson distibution
  *
  * References
  * Dennis et al. (2015) Computational aspects of N-mixture models.
  *   Biometrics 71:237--246. DOI:10.1111/biom.12246
  * Stan users mailing list
  *   https://groups.google.com/forum/#!topic/stan-users/9mMsp1oB69g
  *
  * @param n          Number of observed individuals
  * @param log_lambda Log of Poisson mean of population size
  * @param logit_p    Logit of detection probability
  *
  * return Log probability
  */
  real bivariate_poisson_log_lpmf(int[] n, real log_lambda, real[] p) {
    real s[min(n) + 1];
    if (size(n) != 2) 
      reject("Size of n must be 2.");
    if (p[1] < 0 || p[1] > 1 || p[2] < 0 || p[2] > 1) 
      reject("p must be in [0,1].");
    for (u in 0 : min(n)) 
      s[u + 1] = poisson_log_lpmf(n[1] - u| log_lambda + log(p[1])
                                            + log1m(p[2]))
                 + poisson_log_lpmf(n[2] - u| log_lambda + log(p[2])
                                              + log1m(p[1]))
                 + poisson_log_lpmf(u| log_lambda + log(p[1]) + log(p[2]));
    return log_sum_exp(s);
  }
}
data {
  int<lower=1> R;
  // Number of sites
  int<lower=1> T;
  // Number of replications; fixed as 2
  int<lower=-1> y[R, 2, 7];
  // Counts (-1:NA)
  int<lower=1, upper=7> first[R];
  // First occasion
  int<lower=1, upper=7> last[R];
  // Last occasion
  int<lower=0> K;
  // Upper bounds of population size
}
transformed data {
  int<lower=0> max_y[R, 7];
  int<lower=0, upper=R> num_obs_site[7];
  for (i in 1 : R) {
    for (k in 1 : (first[i] - 1)) 
      max_y[i, k] = 0;
    for (k in (last[i] + 1) : 7) 
      max_y[i, k] = 0;
    for (k in first[i] : last[i]) 
      max_y[i, k] = max(y[i, 1 : T, k]);
  }
  for (k in 1 : 7) {
    num_obs_site[k] = 0;
    for (i in 1 : R) 
      num_obs_site[k] = num_obs_site[k] + (y[i, 1, k] != -1);
  }
}
parameters {
  vector<upper=7>[7] alpha_lam;
  // Constraint for stability
  vector[7] beta;
  vector<upper=7>[R] eps_raw;
  // Constraint for stability
  // Without these constraints, some
  // estimates become unstable maybe due
  // to insufficient information in the
  // data used in the BPA book.
  real<lower=0> sd_lam;
  real<lower=0> sd_p;
  vector<lower=-7, upper=7>[7] logit_p[R, T];
  // Originally `lp' in the BPA book
}
transformed parameters {
  vector[R] eps;
  // Abundance noise
  matrix[R, 7] log_lambda;
  eps = sd_lam * eps_raw;
  for (k in 1 : 7) 
    for (i in 1 : R) 
      log_lambda[i, k] = alpha_lam[k] + eps[i];
}
model {
  // Priors
  alpha_lam ~ normal(0, sqrt(10));
  beta ~ normal(0, sqrt(10));
  eps_raw ~ normal(0, 1);
  // Weakly informative priors are used on sd_lam and sd_p,
  // instead of uniform(0, 3) used in the book
  sd_lam ~ normal(1.5, 0.75);
  sd_p ~ normal(1.5, 0.75);
  for (i in 1 : R) 
    for (j in 1 : T) 
      logit_p[i, j] ~ normal(beta, sd_p);
  // Likelihood
  for (i in 1 : R) 
    for (k in first[i] : last[i]) 
      y[i, 1 : 2, k] ~ bivariate_poisson_log(log_lambda[i, k],
                                             inv_logit(logit_p[i, 1 : 2, k]));
}
generated quantities {
  int totalN[7];
  // Total pop. size across all sites
  vector[7] mean_abundance;
  vector[7] mean_N;
  vector[7] mean_detection;
  real fit = 0;
  real fit_new = 0;
  {
    int N[R, 7];
    // Abundance
    real eval[R, T, 7];
    // Expected values
    real y_new[R, T, 7];
    // Replicate data
    vector[7] p[R, T];
    matrix[T, 7] E[R];
    matrix[T, 7] E_new[R];
    matrix[R, 7] ik_p;
    // Initialize N and ik_p
    N = rep_array(0, R, 7);
    ik_p = rep_matrix(0, R, 7);
    // Initialize E and E_new
    E[1] = rep_matrix(0, T, 7);
    E_new[1] = rep_matrix(0, T, 7);
    for (i in 2 : R) {
      E[i] = E[i - 1];
      E_new[i] = E_new[i - 1];
    }
    for (i in 1 : R) {
      for (j in 1 : T) 
        p[i, j] = inv_logit(logit_p[i, j]);
      for (k in first[i] : last[i]) {
        vector[K + 1] lp;
        for (n in 0 : (max_y[i, k] - 1)) 
          lp[n + 1] = negative_infinity();
        for (n in max_y[i, k] : K) 
          lp[n + 1] = poisson_log_lpmf(n| log_lambda[i, k])
                      + binomial_lpmf(y[i, 1 : T, k]| n, p[i, 1 : T, k]);
        N[i, k] = categorical_rng(softmax(lp)) - 1;
        for (j in 1 : T) {
          // Assess model fit using Chi-squared discrepancy
          // Compute fit statistic E for observed data
          eval[i, j, k] = p[i, j, k] * N[i, k];
          E[i, j, k] = square(y[i, j, k] - eval[i, j, k])
                       / (eval[i, j, k] + 0.5);
          // Generate replicate data and compute fit stats for them
          y_new[i, j, k] = binomial_rng(N[i, k], p[i, j, k]);
          E_new[i, j, k] = square(y_new[i, j, k] - eval[i, j, k])
                           / (eval[i, j, k] + 0.5);
        }
        ik_p[i, k] = mean(p[i, 1 : T, k]);
      }
    }
    for (k in 1 : 7) {
      totalN[k] = sum(N[ : , k]);
      mean_abundance[k] = mean(exp(log_lambda[ : , k]));
      mean_N[k] = 1.0 * totalN[k] / num_obs_site[k];
      mean_detection[k] = sum(ik_p[ : , k]) / num_obs_site[k];
    }
    for (i in 1 : R) {
      fit = fit + sum(E[i]);
      fit_new = fit_new + sum(E_new[i]);
    }
  }
}

  $ ../../../../../../install/default/bin/stanc --auto-format binmix.stan
// Simplest binomial mixture model
data {
  int<lower=0> R;
  // Number of sites
  int<lower=0> T;
  // Number of temporal replications
  int<lower=0> y[R, T];
  // Counts
  int<lower=0> K;
  // Upper bound of population size
}
transformed data {
  int<lower=0> max_y[R];
  for (i in 1 : R) 
    max_y[i] = max(y[i]);
}
parameters {
  real<lower=0> lambda;
  // Mean population size
  real<lower=0, upper=1> p;
  // Detection probability
}
model {
  // Priors
  // A half Cauchy prior is used on lambda, instead of
  // gamma(0.005, 0.005) or uniform(0, 10) proposed in the book.
  lambda ~ cauchy(0, 10);
  // A flat prior [0, 1] is implicitly used on p.
  // Likelihood
  for (i in 1 : R) {
    vector[K - max_y[i] + 1] lp;
    for (j in 1 : (K - max_y[i] + 1)) 
      lp[j] = poisson_lpmf(max_y[i] + j - 1| lambda)
              + binomial_lpmf(y[i]| max_y[i] + j - 1, p);
    target += log_sum_exp(lp);
  }
}

  $ ../../../../../../install/default/bin/stanc --auto-format binmix_cov.stan
// Binomial mixture model with covariates
data {
  int<lower=0> R;
  // Number of sites
  int<lower=0> T;
  // Number of temporal replications
  int<lower=0> y[R, T];
  // Counts
  vector[R] X;
  // Covariate
  int<lower=0> K;
  // Upper bound of population size
}
transformed data {
  int<lower=0> max_y[R];
  for (i in 1 : R) 
    max_y[i] = max(y[i]);
}
parameters {
  real alpha0;
  real alpha1;
  real beta0;
  real beta1;
}
transformed parameters {
  vector[R] log_lambda;
  // Log population size
  matrix[R, T] logit_p;
  // Logit detection probability
  log_lambda = alpha0 + alpha1 * X;
  logit_p = rep_matrix(beta0 + beta1 * X, T);
}
model {
  // Priors
  // Improper flat priors are implicitly used on
  // alpha0, alpha1, beta0 and beta1.
  // Likelihood
  for (i in 1 : R) {
    vector[K - max_y[i] + 1] lp;
    for (j in 1 : (K - max_y[i] + 1)) 
      lp[j] = poisson_log_lpmf(max_y[i] + j - 1| log_lambda[i])
              + binomial_logit_lpmf(y[i]| max_y[i] + j - 1, logit_p[i]);
    target += log_sum_exp(lp);
  }
}
generated quantities {
  int N[R];
  int totalN;
  for (i in 1 : R) 
    N[i] = poisson_log_rng(log_lambda[i]);
  totalN = sum(N);
}

